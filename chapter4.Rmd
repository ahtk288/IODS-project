# Exercise 4. Clustering and classification

*This week Linear Discriminant Analysis (LDA) and K-means clustering was used to study the Boston housing dataset.* 

*LDA is commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications. Dataset is projected onto a lower-dimensional space with good class-separability in order avoid overfitting and also reduce computational costs.([Sebastian Raschka 2020](https://sebastianraschka.com/Articles/2014_python_lda.html#principal-component-analysis-vs-linear-discriminant-analysis))*

*Kmeans algorithm is an iterative algorithm that tries to partition the dataset into Kpre-defined distinct non-overlapping subgroups (clusters) where each data point belongs to only one group. It tries to make the intra-cluster data points as similar as possible while also keeping the clusters as different (far) as possible.  ([Imad Dabbura 2018](https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a)).*


**TASK 2, structure and dimensions of the data**

Boston dataset contains housing values of suburban areas in Boston. The dataframe has 506 observations of 14 variables. Variables are described in the following 
[link](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html)


```{r}
date()

# TASK 2

# access the MASS package
library(MASS)

# load the data
data("Boston")

# explore the dataset
str(Boston)


```


**TASK 3, summary of the data and correlations**

In original Boston dataset variable means ranged from 0.07 (chas) to 408 (tax). The highest correlation was between variabless rad (index of accessibility of radial highways) and tax (full-value property-tax rate per \$10,000). The higher the index of accessibility of radial highway gets the higher property tax rate got. There was also high correlation between concentration of nitrogen oxides and distance to employment center. The higher the distance the lower the concentrations. 


```{r}

# TASK 3

library(corrplot)
library(tidyr)

#summary of the data
summary(Boston)

# calculate the correlation matrix and round it
cor_matrix <- cor(Boston) %>% round(digits = 2)

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)

#https://www.statsandr.com/blog/correlogram-in-r-how-to-highlight-the-most-correlated-variables-in-a-dataset/
# devtools::install_github("laresbernardo/lares")
library(lares)

corr_cross(Boston, # name of dataset
  max_pvalue = 0.05, # display only significant correlations (at 5% level)
  top = 10 # display top 10 couples of variables (by correlation coefficient)
)

```


**TASK 4, scaling the data and creating test data sets**

After scaling the data all variable means were 0. 


```{r}

# TASK 4

# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# summary of the scaled crime rate
summary(boston_scaled$crim)
```


Categorical variable of the crime was created using scaled data and quantiles as break points. Original crime rate was replaced with the new categorical variable. Further dataset was divided to train and test sets. 


```{r}
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)
```


**TASK 5, linear discriminant analysis (LDA)**

LDA can be visualized with LDA biplot. LDA finds a combination of the variables that separate the target variable classes. High crime rate was clearly separated from other categories. Categories low, med_low and med_high were more or less overlapping each other.


```{r}
# TASK 5, linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)


```


**TASK 6, LDA model predictions**

The crime rate classes were predicted with the LDA model and the results were cross tabulated.

The prediction error of the LDA model was 0.3, so the accuracy of the model was 70 %. This means that 71 out of 102 observations were correctly predicted to correct crime categories.


```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

# tabulate the target variable versus the predictions
table(high_use = correct_classes, prediction = lda.pred$class) %>% prop.table %>% addmargins


```


**TASK 7, distances between variables**

Distances between the scaled Boston dataset variables were calculated (Euclidean and Manhattan distance). Mean of the Euclidean distance was 4.9 and the Manhattan distance 13.5. 


```{r}
library(MASS)

data('Boston')

# center and standardize variables
boston_scaled <- scale(Boston)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# euclidean distance matrix
dist_eu <- dist(boston_scaled)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(boston_scaled, method = 'manhattan')

# look at the summary of the distances
summary(dist_man)

```


**K-means clustering**

With kmeans clustering we can calculate the distance matrix automatically. 

One way to determine optimal number of clusters is to to look at how the total of within cluster sum of squares (WCSS) behaves when the number of cluster changes. According to datacamp the optimal number of clusters is when the total WCSS drops radically. The most radical drop stopped at cluster 2 where an elbow is formed so optimal number of clusters would be then 2. Drop still continues in the plot TWCSS and starts flatten out at cluster 6 onwards. So I decided to try two models with 2 and 6 clusters.

The K Means cluster plots (with colors) shows how well clusters differ from each other. More separate the better. 


```{r}
set.seed(123) # to prevent K-means producing different results every time

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
library(ggplot2)
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <-kmeans(boston_scaled, centers = 2)

# plot the scaled Boston dataset with clusters
library(GGally)
ggpairs(boston_scaled, upper = list(continuous = wrap("cor", size = 2)))
pairs(boston_scaled, col = km$cluster)

# k-means clustering
km <-kmeans(boston_scaled, centers = 6)

# plot the scaled Boston dataset with clusters
library(GGally)
ggpairs(boston_scaled, upper = list(continuous = wrap("cor", size = 2)))
pairs(boston_scaled, col = km$cluster)
```

